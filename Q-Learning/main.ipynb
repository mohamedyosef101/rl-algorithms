{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedyosef101/rl-algorithms/blob/main/Q-Learning/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Algorithm\n",
        "At first, I assume that you have alread read my article titled \"[Reinforcement Learning: All the Basics](https://mohamedyosef101.github.io/publication/rl101)\" or you know what is RL and Q-learning.\n"
      ],
      "metadata": {
        "id": "_n9_maAcZTO3"
      },
      "id": "_n9_maAcZTO3"
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries we will use\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "Tn0a7ZdWat74"
      },
      "id": "Tn0a7ZdWat74",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll use `numpy` for its ability to work with matices and arrays. While `random` is simply used to generate random number to tell us when to explore and when to exploit (a simple solution for [exploration/exploitation trade-off](https://mohamedyosef101.github.io/publication/rl101/#13-the-explorationexploitation-trade-off))."
      ],
      "metadata": {
        "id": "V8NyZH6Baz_g"
      },
      "id": "V8NyZH6Baz_g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0. Set up the environment"
      ],
      "metadata": {
        "id": "ozZjJXt4aKX4"
      },
      "id": "ozZjJXt4aKX4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wil use a basic 5 by 5 grid world as our environment. Our agent should reach the goal in point (4, 4) without goint to any of the obstacles in the grid."
      ],
      "metadata": {
        "id": "gXla7BKmaq0v"
      },
      "id": "gXla7BKmaq0v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grid world\n",
        "grid_size = (5, 5)\n",
        "goal_state = (4, 4)\n",
        "start_state = (0, 0)\n",
        "obstacles = [(1, 1), (2, 2), (3, 3)]\n",
        "actions = ['up', 'down', 'right', 'left']\n",
        "action_space = [0, 1, 2, 3]\n",
        "# I have turned actions from str to int for easier calculations"
      ],
      "metadata": {
        "id": "sSkTglR0dcp_"
      },
      "id": "sSkTglR0dcp_",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's show you how our grid world looks like..."
      ],
      "metadata": {
        "id": "nRrlIfC0esii"
      },
      "id": "nRrlIfC0esii"
    },
    {
      "cell_type": "code",
      "source": [
        "def print_grid(state, goal_state, obstacles):\n",
        "  \"\"\"Function to print the grid with the agent's position.\n",
        "  \"\"\"\n",
        "  grid = np.zeros(grid_size, dtype=str)\n",
        "  grid[:] = '.'\n",
        "  grid[goal_state] = 'G'\n",
        "  for obs in obstacles:\n",
        "    grid[obs] = 'X'\n",
        "  grid[state] = 'A'\n",
        "  for row in grid:\n",
        "    print(' '.join(row))\n",
        "  print()"
      ],
      "metadata": {
        "id": "coom4h1rexZt"
      },
      "id": "coom4h1rexZt",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_grid((0, 0), goal_state, obstacles)"
      ],
      "metadata": {
        "id": "9MJXeuUue-99",
        "outputId": "afbd409e-5339-4f5a-d695-ce9cff319b95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9MJXeuUue-99",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A . . . .\n",
            ". X . . .\n",
            ". . X . .\n",
            ". . . X .\n",
            ". . . . G\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Environment dynamics\n",
        "Here, we will say how the environment works and responses to the agent. I tried to make this code similar to what you can get in [Gymnasium](https://gymnasium.farama.org/api/env/) so you can also work with Gym enviroments if you need (and you will need it, trust me!)."
      ],
      "metadata": {
        "id": "prtFcMJVfLTn"
      },
      "id": "prtFcMJVfLTn"
    },
    {
      "cell_type": "code",
      "source": [
        "def take_step(state, action, max_steps, step_count):\n",
        "    \"\"\"\n",
        "    Takes the current state and the agent's action and returns\n",
        "    the next state, reward, and tell if the process terminated or truncated,\n",
        "    also it might give you some more info represented in a dictionary.\n",
        "\n",
        "    Args:\n",
        "        state (tuple): Current position of the agent (row, col).\n",
        "        action (str): One of ['up', 'down', 'left', 'right'].\n",
        "        max_steps (int): The max number of steps taken in each episode\n",
        "        it is recommended to use the same max_steps in training\n",
        "        step_count (int): count how many steps has the agent taken so far\n",
        "        I put it here just to determin if the process truncated or not.\n",
        "\n",
        "    Returns:\n",
        "        next_state (tuple): New position of the agent.\n",
        "        reward (float): Reward received for this action.\n",
        "        terminated (bool): Whether the goal reached or obstacle hit.\n",
        "        truncated (bool): Whether the episode was truncated (e.g., due to a steps limit).\n",
        "        info (dict): Additional information (optional, can be empty for now).\n",
        "    \"\"\"\n",
        "\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    info = {}\n",
        "\n",
        "    # --- Calculate the potential next state based on the action ---\n",
        "    x = 0\n",
        "    y = 0\n",
        "    x, y = state\n",
        "    if action == 0: # up\n",
        "        x = max(0, x - 1)\n",
        "    elif action == 1: # down\n",
        "        x = min(grid_size[0] - 1, x + 1)\n",
        "    elif action == 2: # right\n",
        "        y = min(grid_size[0] - 1, y + 1)\n",
        "    elif action == 3: # left\n",
        "        y = max(0, y - 1)\n",
        "    next_state = (x, y)\n",
        "\n",
        "    # --- Check for Obstacles and Goal ---\n",
        "    if next_state in obstacles: # Obstacle state\n",
        "        terminated = True\n",
        "        reward = -100.0\n",
        "    elif next_state == goal_state:\n",
        "        terminated = True\n",
        "        reward = 100.0\n",
        "    else:\n",
        "      reward = -1.0\n",
        "\n",
        "    # --- Check for Truncation (max steps reached) ---\n",
        "    if step_count >= max_steps and not terminated:\n",
        "        truncated = True\n",
        "\n",
        "    return next_state, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "dSHPfHa1geOm"
      },
      "id": "dSHPfHa1geOm",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have create the reward system so that the agent gets a reward of 100 in case of reaching the goal and -100 in case of reaching an obstacle while -1 otherwise."
      ],
      "metadata": {
        "id": "enQ2DnsZ5iL0"
      },
      "id": "enQ2DnsZ5iL0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Initialize Q-Table"
      ],
      "metadata": {
        "id": "VgmwLnJtiLsE"
      },
      "id": "VgmwLnJtiLsE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning is a model-free, off-policy reinforcement learning algorithm. So, rather than learning a [value estimate](https://mohamedyosef101.github.io/publication/rl101/#14-two-approaches-for-solving-rl-problems), we learn action-value or the quality of the action, $Q(s,a)$, and to get the expected future reward of starting in state $s_t$ and taking action $a_t$ by using a [Temporal Difference](https://mohamedyosef101.github.io/publication/rl101/#24-temporal-difference-learning) (TD) learning approach and [Bellman equation](https://mohamedyosef101.github.io/publication/rl101/#21-the-bellman-equations) to optimize the value function:\n",
        "\n",
        "$$\n",
        "Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha [R_{t+1} +\n",
        "\\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is the learning rate, $\\gamma$ is the discount factor."
      ],
      "metadata": {
        "id": "TkaHEkJlqQSX"
      },
      "id": "TkaHEkJlqQSX"
    },
    {
      "cell_type": "code",
      "source": [
        "def Qtable0(grid_size, action_space):\n",
        "  Qtable = np.zeros((grid_size[0], grid_size[1], len(action_space)))\n",
        "  return Qtable\n",
        "\n",
        "# create it for our environment\n",
        "grid_Qtable = Qtable0(grid_size, action_space)"
      ],
      "metadata": {
        "id": "OKBBmo3Wu8aJ"
      },
      "id": "OKBBmo3Wu8aJ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Epsilon-Greedy Policy"
      ],
      "metadata": {
        "id": "PkpDSd3dvSMS"
      },
      "id": "PkpDSd3dvSMS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the [exploration/exploitation trade-off](https://mohamedyosef101.github.io/publication/rl101/#13-the-explorationexploitation-trade-off), the agent doesn't know when to explore and when to exploit. Therefore, Q-learning often uses an epsilon-greedy policy (ϵ-greedy) in which the agent exploits '1-ϵ' times by taking the best action according to the agent information and explore ϵ times by taking a random action.\n",
        "\n",
        "The greedy policy which we will use to exploit is used to select the action with the best Q-value in the Q-table.\n",
        "\n",
        "$$\n",
        "\\text{greedy policy : } \\\\\n",
        "\\pi^*(s) = \\arg \\max_a Q^*(s,a)\n",
        "$$"
      ],
      "metadata": {
        "id": "8y6ohg2Svhl0"
      },
      "id": "8y6ohg2Svhl0"
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "  return action"
      ],
      "metadata": {
        "id": "I7lRLsYlxFxG"
      },
      "id": "I7lRLsYlxFxG",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  random_num = random.uniform(0, 1)\n",
        "  if random_num > epsilon:\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  else:\n",
        "    action = random.choice(action_space)\n",
        "  return action"
      ],
      "metadata": {
        "id": "eKweSkZcxKNd"
      },
      "id": "eKweSkZcxKNd",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Define parameters"
      ],
      "metadata": {
        "id": "8T1VOJgExWI_"
      },
      "id": "8T1VOJgExWI_"
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1   # Learning rate\n",
        "gamma = 0.99  # Dicount factor\n",
        "epsilon = 0.1   # Exploration rate\n",
        "max_steps = 25  # max number of steps per episode\n",
        "\n",
        "num_episodes = 1000  # Number of training episodes\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.05\n",
        "decay_rate = 0.005\n",
        "\n",
        "eval_episodes = 100   # Number of evaluation episodes"
      ],
      "metadata": {
        "id": "Cw7EM2MFxbdG"
      },
      "id": "Cw7EM2MFxbdG",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation**.\n",
        "\n",
        "$$\n",
        "\\epsilon = \\epsilon_{\\min} + (\\epsilon_{\\max} - \\epsilon_\\min)\\ \\times \\ e^{- \\text{ decay rate} \\times \\text{episode}}\n",
        "$$"
      ],
      "metadata": {
        "id": "OHUtbTC4xPw4"
      },
      "id": "OHUtbTC4xPw4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Start the training"
      ],
      "metadata": {
        "id": "B9sFCem4z7xW"
      },
      "id": "B9sFCem4z7xW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Algorithm**: Q-Learning\n",
        "\n",
        "---\n",
        "\n",
        "**Input**: num_episodes (positive int), learning rate $\\alpha$ (small positive fraction), Discount factor $\\gamma$ (a fraction close to 1),\n",
        " $\\epsilon_\\min$ (small positive fraction), $\\epsilon_\\max$ (fraction close to 1), decay_rate (very small positive fraction), max_steps (positive int), Qtable (array of q-values).\n",
        "\n",
        " <br>\n",
        "\n",
        "**Output**: value function $Q$ ($\\approx q_\\pi$ if num_episodes is large enough)\n",
        "\n",
        "<br>\n",
        "\n",
        "Initialize Q arbitrarily (e.g., $Q(s,a) = 0$ for all $s \\in \\mathcal S$ and $a \\in \\mathcal A(s)$, and $Q(\\text{terminal-state}, \\cdot) = 0)$\n",
        "\n",
        "* **for** $i \\gets 1$ **to** num_episodes **do** <br>\n",
        "  * $\\epsilon \\gets \\epsilon_{\\min} + (\\epsilon_{\\max} - \\epsilon_\\min)\\ \\times \\ e^{- \\text{ decay rate} \\times \\text{episode}}$\n",
        "  * Observe $s_0$\n",
        "  * $t \\text{ (or step)} \\gets 0$\n",
        "  * **repeat**\n",
        "    * Choose action $a_t$ using policy derived from Q (e.g., epsilon-greedy, greedy policy)\n",
        "    * Take action $a_t$ and observe $r_{t+1}, s_{t+1}$\n",
        "    * $Q_{\\text{new}}(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha [r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]$\n",
        "    * $t \\gets t+1$\n",
        "  * **until** $s_t$ is terminal;\n",
        "* **EndFor**\n",
        "* **return** $Q$\n",
        "---"
      ],
      "metadata": {
        "id": "k9vWQu1S0HaB"
      },
      "id": "k9vWQu1S0HaB"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_episodes, alpha, gamma, max_epsilon,\n",
        "          min_epsilon, decay_rate, max_steps, Qtable):\n",
        "  for episode in range(num_episodes):\n",
        "\n",
        "    # reduce epsilon\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp( - decay_rate)\n",
        "\n",
        "    # reset the environment\n",
        "    step = 0\n",
        "    state = start_state\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "      new_state, reward, terminated, truncated, info = take_step(state, action, max_steps, step)\n",
        "\n",
        "      # update Qtable\n",
        "      Qtable[state][action] = Qtable[state][action] + alpha * (\n",
        "          reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]\n",
        "      )\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "      step += 1\n",
        "    episode += 1\n",
        "  return Qtable"
      ],
      "metadata": {
        "id": "W2ztVE7g4zDh"
      },
      "id": "W2ztVE7g4zDh",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "Qtable = train(num_episodes, alpha, gamma, max_epsilon,\n",
        "          min_epsilon, decay_rate, max_steps, grid_Qtable)"
      ],
      "metadata": {
        "id": "17X_iBhl41yz",
        "outputId": "8b9d18a4-8627-4c2a-9e2b-ccc4f18d7072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "17X_iBhl41yz",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 131 ms, sys: 1.01 ms, total: 132 ms\n",
            "Wall time: 141 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6. Evaluate the result"
      ],
      "metadata": {
        "id": "GzAk7NcX5J6F"
      },
      "id": "GzAk7NcX5J6F"
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_agent(max_steps, eval_episdoes, Qtable):\n",
        "  episode_rewards = []\n",
        "  for episode in range(eval_episodes):\n",
        "    state = start_state\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      action = greedy_policy(Qtable, state)\n",
        "      new_state, reward, terminated, truncated, info = take_step(state, action, max_steps, step)\n",
        "      total_rewards_ep += reward\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "    episode += 1\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "v39-718H5Vvw"
      },
      "id": "v39-718H5Vvw",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = eval_agent(max_steps, eval_episodes, Qtable)\n",
        "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "P1e_t9WO5Ylk",
        "outputId": "0cad486b-081b-49a1-c119-716e2f0b810c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "P1e_t9WO5Ylk",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward = 93.00 +/- 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This result means that our agent consistently reaches the goal within 7 moves or steps (`100 - 7 = 93`) and `+\\- 0.00` mean that there is no standard divation which means there is no variation in the agent's performance across episodes. This consistent performance implies deterministic behavior, leading to the same outcome each time the environment is navigated."
      ],
      "metadata": {
        "id": "FIG-9vSw51PG"
      },
      "id": "FIG-9vSw51PG"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}