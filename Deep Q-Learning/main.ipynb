{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedyosef101/rl-algorithms/blob/main/Deep%20Q-Learning/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-learning\n",
        "\n",
        "---\n",
        "source: Maxim Lapan. [Deep Reinforcement Learning Hands-On](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition), 2018.\n",
        "\n",
        "---\n",
        "\n",
        "1. Initialize $Q(s,a)$ with some initial approximation\n",
        "2. By interacting with the environment, obtain the tuple $(s, a, r, s')$\n",
        "3. Caclulate loss: $L = (Q_{s,a} - r)^2$ if episode has ended or $L = (Q_{s,a} - (r+\\gamma \\max_{a' \\in \\mathcal{A}} Q_{s',a'}))^2$ otherwise\n",
        "4. Update $Q(s, a)$ using the [stochastic gradient decent (SGD)](https://youtu.be/vMh0zPT0tLI?si=8N6DzFw-616Tixof) algorithm, by minimizing the loss with respect to the model parameters\n",
        "5. Repeat from step 2 until converged\n",
        "\n",
        "### But this formula have problems:\n",
        "- exploration/epolitation trade-off (solved by using epsilon-greedy policy)\n",
        "* training data is not i.i.d which is a fundamental requirement for SGD optimization (solved by using a replay buffer)\n",
        "* Neural network cannot distinguish between $Q(s,a)$ and $Q(s', a')$. This can make our training really unstable, like chasing our own tail. (solved by using a target network)\n",
        "* temporal difference where one single frame is not enough to capture all important information (POMDP). So, we have two options: to work with POMDP or turn it into MDP by using k-frames (commonly 4).\n",
        "\n",
        "### The final form of DQN training\n",
        "The basic DQN uses epsilon-greedy, replay buffer, and target network.\n",
        "\n",
        "The original paper (without target network) was published at the end of 2013 (Playing Atari with Deep Reinforcement Learning 1312.5602v1, Mnih and others.), and they used seven games for testing. Later, at the beginning of 2015, a revised version of the article, with 49 different games, was published in Nature (Human-Level Control Through Deep Reinforcement Learning doi:10.1038/nature14236, Mnih and others.)\n",
        "\n",
        "The algorithm for DQN from the preceding papers has the following steps:\n",
        "1. Initialize parameters for $Q(s, a)$ and $\\hat{Q}(s,a)$ with random weights, $\\epsilon \\gets 1.0$, and empty replay buffer\n",
        "2. With probability Ïµ, select a random action $a$, otherwise $a=\\arg \\max_a Q_{s,a}$\n",
        "3. Execute action $a$ in an emulator and observe reward $r$ and the next state $s'$\n",
        "4. Store transition $(s, a, r, s')$ in the replay buffer\n",
        "5. Sample a random minibatch of transitions from the replay buffer\n",
        "6. For every transitions in the buffer, calculate target $y=r$ if the episode has ended at this step or $y=r + gamma \\max_{a'} \\in \\mathcal{A} \\hat{Q}_{s', a'}$ otherwise\n",
        "7. Calculate loss: $L = (Q_{s,a} - y)^2$\n",
        "8. Update $Q(s,a)$ using the SGD algorithm by minimizing the loss in respect to model parameters\n",
        "9. Every N steps copy weights from $Q$ to $\\hat{Q_t}$\n",
        "10. Repeat from step 2 until converged"
      ],
      "metadata": {
        "id": "p0kIthFL7QXE"
      },
      "id": "p0kIthFL7QXE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}